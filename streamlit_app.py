import streamlit as st
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from transformers import pipeline

# ğŸ’¡ ë‰´ìŠ¤ í¬ë¡¤ë§ í•¨ìˆ˜
def get_naver_news(keywords, start_date, end_date):
    """ë„¤ì´ë²„ ë‰´ìŠ¤ì—ì„œ í‚¤ì›Œë“œ ì¡°í•©ìœ¼ë¡œ ìµœëŒ€ 7ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ ì œëª©ê³¼ ë§í¬ë¥¼ ëŒ“ê¸€ ìˆ˜, ë°œí‘œ ì‹œê°„, ì¶œì²˜ ì •ë³´ì™€ í•¨ê»˜ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    news_list = []
    
    for keyword in keywords:
        url = (
            f"https://search.naver.com/search.naver?where=news&query={keyword}"
            f"&pd=4&ds={start_date}&de={end_date}"
        )
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')
        articles = soup.select('.news_tit')  # ë‰´ìŠ¤ ì œëª© ì„ íƒì
        
        # ë‰´ìŠ¤ ì •ë³´ ìˆ˜ì§‘
        for article in articles:
            title = article.get_text()
            link = article['href']
            comment_count = 0  # ëŒ“ê¸€ ìˆ˜ ì´ˆê¸°í™”
            time = article.find_next('span', {'class': 'info'})  # ë°œí‘œ ì‹œê°„
            source = article.find_next('a', {'class': 'info_nclicks'})  # ì¶œì²˜

            # ëŒ“ê¸€ ìˆ˜ ì¶”ì¶œ
            comment_section = article.find_next('a', {'class': 'nclicks(fls.comment)'})
            if comment_section:
                comment_count = int(comment_section.get_text().replace(',', ''))  # ìˆ«ìë§Œ ì¶”ì¶œí•˜ì—¬ ì •ìˆ˜í˜•ìœ¼ë¡œ ë³€í™˜
                
            if time:
                time = time.get_text()  # ê¸°ì‚¬ ë°œí‘œ ì‹œê°„
            else:
                time = 'ë°œí‘œ ì‹œê°„ ì—†ìŒ'

            if source:
                source = source.get_text()  # ë‰´ìŠ¤ ì¶œì²˜
            else:
                source = 'ì¶œì²˜ ì—†ìŒ'
                
            # ë‰´ìŠ¤ ë³¸ë¬¸ ìš”ì•½ ì¶”ê°€
            summary = get_news_summary(link)
            
            news_list.append((title, link, comment_count, time, source, summary))
            
            # ìµœëŒ€ 7ê°œì˜ ë‰´ìŠ¤ë§Œ ê°€ì ¸ì˜¤ê¸°
            if len(news_list) >= 7:
                break
        
        if len(news_list) >= 7:
            break

    # ëŒ“ê¸€ ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    sorted_news = sorted(news_list, key=lambda x: x[2], reverse=True)[:7]
    return sorted_news

# ë‰´ìŠ¤ ë³¸ë¬¸ì„ ë‘ ê°€ì§€ ìˆ˜ì¤€ìœ¼ë¡œ ìš”ì•½í•˜ëŠ” í•¨ìˆ˜
def get_news_summary(link):
    """ë‰´ìŠ¤ ë³¸ë¬¸ì„ ê°€ì ¸ì™€ ìš”ì•½í•©ë‹ˆë‹¤."""
    try:
        response = requests.get(link)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ë‰´ìŠ¤ ë³¸ë¬¸ ì¶”ì¶œ (ë„¤ì´ë²„ ë‰´ìŠ¤ì˜ ë³¸ë¬¸ì€ 'div'íƒœê·¸ì— í¬í•¨ë¨)
        content = soup.select_one('.newsct_article')  # ê¸°ì‚¬ ë³¸ë¬¸
        if content:
            full_text = content.get_text().strip()
            
            # ìš”ì•½ ëª¨ë¸ ë¡œë“œ (ì²« ë²ˆì§¸ ìš”ì•½: ê°„ëµí•œ ìš”ì•½)
            summarizer = pipeline("summarization")
            summary_short = summarizer(full_text[:1000], max_length=150, min_length=50, do_sample=False)[0]['summary_text']
            
            # ë‘ ë²ˆì§¸ ìš”ì•½: ë” ê¸´ ìš”ì•½
            summary_long = summarizer(full_text[:2000], max_length=300, min_length=100, do_sample=False)[0]['summary_text']
            
            return summary_short, summary_long
        else:
            return 'ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', 'ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
    except Exception as e:
        return 'ìš”ì•½ ì‹¤íŒ¨', 'ìš”ì•½ ì‹¤íŒ¨'

# Streamlit App
st.title("ESG & Network Insights Weekly News")

# 1. ESG Trends Summary
with st.expander("ê¸ˆì£¼ì˜ ESG íŠ¸ë Œë“œ ìš”ì•½"):
    st.write("ì—¬ê¸°ì— ê¸ˆì£¼ì˜ ESG íŠ¸ë Œë“œ ìš”ì•½ ë‚´ìš©ì„ ì¶”ê°€í•˜ì„¸ìš”.")

# 2. Global & Local News
with st.expander("êµ­ë‚´ì™¸ ì£¼ìš” ë‰´ìŠ¤"):
    st.write("ì£¼ìš” í‚¤ì›Œë“œë¥¼ ì„ íƒí•˜ì„¸ìš” (ìµœëŒ€ 7ê°œ ë‰´ìŠ¤ í‘œì‹œ).")

    # í‚¤ì›Œë“œ ì„ íƒ (ê°€ë¡œ ë°°ì¹˜)
    keywords = ["í™˜ê²½", "ì‚¬íšŒ", "ê±°ë²„ë„ŒìŠ¤", "ê¸°ìˆ ", "ê²½ì œ", "ì •ì¹˜", "ê¸€ë¡œë²Œ íŠ¸ë Œë“œ"]
    cols = st.columns(len(keywords))
    selected_keywords = [kw for i, kw in enumerate(keywords) if cols[i].checkbox(kw)]

    # ğŸ”„ ì¼ì£¼ì¼ ìë™ í•„í„° ì„¤ì •
    end_date = datetime.today()  # ì˜¤ëŠ˜ ë‚ ì§œ
    start_date = end_date - timedelta(days=7)  # ì¼ì£¼ì¼ ì „ ë‚ ì§œ

    st.write(f"ë‰´ìŠ¤ ê²€ìƒ‰ ê¸°ê°„: {start_date.strftime('%Y.%m.%d')} ~ {end_date.strftime('%Y.%m.%d')}")

    # ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤í–‰
    if st.button("ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"):
        if selected_keywords:
            news = get_naver_news(selected_keywords, start_date.strftime('%Y.%m.%d'), end_date.strftime('%Y.%m.%d'))
            if news:
                st.write("### ê²€ìƒ‰ëœ ë‰´ìŠ¤ ëª©ë¡:")
                for title, link, comment_count, time, source, (summary_short, summary_long) in news:
                    st.write(f"**ì œëª©**: [{title}]({link})")
                    st.write(f"- **ëŒ“ê¸€ ìˆ˜**: {comment_count}")
                    st.write(f"- **ë°œí‘œ ì‹œê°„**: {time}")
                    st.write(f"- **ì¶œì²˜**: {source}")
                    st.write(f"- **ê°„ëµ ìš”ì•½**: {summary_short}")
                    st.write(f"- **ìì„¸í•œ ìš”ì•½**: {summary_long}")
                    st.write("---")
            else:
                st.write("í•´ë‹¹ ê¸°ê°„ì— ê´€ë ¨ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.write("í‚¤ì›Œë“œë¥¼ ì„ íƒí•˜ì„¸ìš”.")

# 3. SKT Network Insights
with st.expander("ì¸ì‚¬ì´íŠ¸ for SKT Network"):
    st.write("ì—¬ê¸°ì— SKT Network ê´€ë ¨ ì¸ì‚¬ì´íŠ¸ ë‚´ìš©ì„ ì¶”ê°€í•˜ì„¸ìš”.")

# Footer
st.markdown("**Weekly News Collection App** - ESG & Network Insights")
